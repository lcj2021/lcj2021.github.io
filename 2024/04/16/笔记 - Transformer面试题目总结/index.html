

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Channing Lau">
  <meta name="keywords" content="">
  
    <meta name="description" content="1.Attention1.1 讲讲对Attention的理解？Attention机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。 核心思想是在处理序列数据时，网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息。 在序列建模任务中，比如机器翻译、文本摘要、语言理解等，输入序列">
<meta property="og:type" content="article">
<meta property="og:title" content="笔记 - Transformer面试题目总结">
<meta property="og:url" content="https://lcj2021.github.io/2024/04/16/%E7%AC%94%E8%AE%B0%20-%20Transformer%E9%9D%A2%E8%AF%95%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="ChanningLau&#39;s harbour">
<meta property="og:description" content="1.Attention1.1 讲讲对Attention的理解？Attention机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。 核心思想是在处理序列数据时，网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息。 在序列建模任务中，比如机器翻译、文本摘要、语言理解等，输入序列">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://dongnian.icu/media/image.png">
<meta property="og:image" content="https://dongnian.icu/media/image_1.png">
<meta property="og:image" content="https://dongnian.icu/media/image_2.png">
<meta property="og:image" content="https://dongnian.icu/media/image_3.png">
<meta property="og:image" content="https://dongnian.icu/media/image_4.png">
<meta property="og:image" content="https://dongnian.icu/media/image_5.png">
<meta property="og:image" content="https://dongnian.icu/media/image_6.png">
<meta property="og:image" content="https://dongnian.icu/media/image_7.png">
<meta property="og:image" content="https://dongnian.icu/media/image_8.png">
<meta property="og:image" content="https://dongnian.icu/media/image_9.png">
<meta property="og:image" content="https://dongnian.icu/media/image_10.png">
<meta property="og:image" content="https://dongnian.icu/media/image_11.png">
<meta property="og:image" content="https://dongnian.icu/media/image_12.png">
<meta property="article:published_time" content="2024-04-15T16:00:00.000Z">
<meta property="article:modified_time" content="2024-07-15T01:25:13.477Z">
<meta property="article:author" content="Channing Lau">
<meta property="article:tag" content="Cheatsheet">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://dongnian.icu/media/image.png">
  
  
  
  <title>笔记 - Transformer面试题目总结 - ChanningLau&#39;s harbour</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"lcj2021.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ChanningLau&#39;s harbour</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="笔记 - Transformer面试题目总结"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-04-16 00:00" pubdate>
          2024年4月16日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          8.3k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          70 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">笔记 - Transformer面试题目总结</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="1-Attention"><a href="#1-Attention" class="headerlink" title="1.Attention"></a>1.Attention</h2><h3 id="1-1-讲讲对Attention的理解？"><a href="#1-1-讲讲对Attention的理解？" class="headerlink" title="1.1 讲讲对Attention的理解？"></a>1.1 讲讲对Attention的理解？</h3><p>Attention机制是一种在处理时序相关问题的时候常用的技术，主要用于处理序列数据。</p>
<p>核心思想是在处理序列数据时，网络应该更关注输入中的重要部分，而忽略不重要的部分，它通过学习不同部分的权重，将输入的序列中的重要部分显式地加权，从而使得模型可以更好地关注与输出有关的信息。</p>
<p>在序列建模任务中，比如机器翻译、文本摘要、语言理解等，输入序列的不同部分可能具有不同的重要性。传统的循环神经网络（RNN）或卷积神经网络（CNN）在处理整个序列时，难以捕捉到序列中不同位置的重要程度，可能导致信息传递不够高效，特别是在处理长序列时表现更明显。</p>
<p>Attention机制的关键是引入一种机制来动态地计算输入序列中各个位置的权重，从而在每个时间步上，对输入序列的不同部分进行加权求和，得到当前时间步的输出。这样就实现了模型对输入中不同部分的关注度的自适应调整。</p>
<h3 id="1-2-Attention的计算步骤是什么？"><a href="#1-2-Attention的计算步骤是什么？" class="headerlink" title="1.2 Attention的计算步骤是什么？"></a>1.2 Attention的计算步骤是什么？</h3><p>具体的计算步骤如下：</p>
<ul>
<li><strong>计算查询（Query）</strong>：查询是当前时间步的输入，用于和序列中其他位置的信息进行比较。</li>
<li><strong>计算键（Key）和值（Value）</strong>：键表示序列中其他位置的信息，值是对应位置的表示。键和值用来和查询进行比较。对于一个句子，对每个词汇进行线性变换，得到变换后的查询（q）、键（k）和值（v）向量表示。</li>
<li><strong>计算注意力权重</strong>：通过将查询和键进行内积运算，然后应用softmax函数，得到注意力权重。这些权重表示了在当前时间步，模型应该关注序列中其他位置的重要程度。</li>
<li><strong>加权求和</strong>：根据注意力权重将值进行加权求和，得到当前时间步的输出。</li>
</ul>
<p>在Transformer中，Self-Attention 被称为”Scaled Dot-Product Attention”，其计算过程如下：</p>
<ol>
<li>对于输入序列中的每个位置，通过计算其与所有其他位置之间的相似度得分（通常通过点积计算）。</li>
<li>对得分进行缩放处理（除以$\sqrt{d_k}$），以防止梯度爆炸。</li>
<li>将得分用softmax函数转换为注意力权重，以便计算每个位置的加权和。</li>
<li>使用注意力权重对输入序列中的所有位置进行加权求和，得到每个位置的自注意输出。</li>
</ol>
<p>$$<br>Attention(Q,K,V)&#x3D;softmax(\frac{QK^T}{\sqrt{d_k}})V<br>$$</p>
<h3 id="1-3-Attention机制和传统的Seq2Seq模型有什么区别？"><a href="#1-3-Attention机制和传统的Seq2Seq模型有什么区别？" class="headerlink" title="1.3 Attention机制和传统的Seq2Seq模型有什么区别？"></a>1.3 Attention机制和传统的Seq2Seq模型有什么区别？</h3><p>Seq2Seq模型是一种基于编码器-解码器结构的模型，主要用于处理序列到序列的任务，例如机器翻译、语音识别等。</p>
<p>传统的Seq2Seq模型<strong>只使用编码器来捕捉输入序列的信息，而解码器只从编码器的最后状态中获取信息，并将其用于生成输出序列</strong>。</p>
<p>而Attention机制则允许<strong>解码器在生成每个输出时，根据输入序列的不同部分给予不同的注意力，从而使得模型更好地关注到输入序列中的重要信息</strong>。</p>
<h3 id="1-4-self-attention-和-target-attention的区别？"><a href="#1-4-self-attention-和-target-attention的区别？" class="headerlink" title="1.4 self-attention 和 target-attention的区别？"></a>1.4 self-attention 和 target-attention的区别？</h3><p>self-attention是指在序列数据中，<strong>将当前位置与其他位置之间的关系建模</strong>。它通过计算每个位置与其他所有位置之间的相关性得分，从而<strong>为每个位置分配一个权重</strong>。这使得模型能够根据输入序列的不同部分的重要性，自适应地选择要关注的信息。</p>
<p>target-attention则是指将<strong>注意力机制应用于目标（或查询）和一组相关对象之间的关系</strong>。它用于将目标与其他相关对象进行比较，并将注意力分配给与目标最相关的对象。这种类型的注意力通常用于任务如机器翻译中的编码-解码模型，其中需要将源语言的信息对齐到目标语言。</p>
<p>因此，<strong>自注意力主要关注序列内部的关系，而目标注意力则关注目标与其他对象之间的关系</strong>。这两种注意力机制在不同的上下文中起着重要的作用，帮助模型有效地处理序列数据和相关任务。</p>
<h3 id="1-5-在常规attention中，一般有k-v，那self-attention-可以吗"><a href="#1-5-在常规attention中，一般有k-v，那self-attention-可以吗" class="headerlink" title="1.5 在常规attention中，一般有k&#x3D;v，那self-attention 可以吗?"></a>1.5 在常规attention中，一般有k&#x3D;v，那self-attention 可以吗?</h3><p>self-attention实际只是attention中的一种特殊情况，因此k&#x3D;v是没有问题的，也即K，V参数矩阵相同。</p>
<p>实际上，在Transformer模型中，Self-Attention的典型实现就是k等于v的情况。<br>k和v共享相同的参数矩阵$W_k$和$W_v$。这样做的好处是可以减少模型的参数量，并且在计算上更加高效。</p>
<p>Transformer中的Self-Attention被称为”Scaled Dot-Product Attention”，其中<strong>通过将词向量进行线性变换来得到Q、K、V，并且这三者是相等的</strong>。</p>
<h3 id="1-6-目前主流的attention方法有哪些？"><a href="#1-6-目前主流的attention方法有哪些？" class="headerlink" title="1.6 目前主流的attention方法有哪些？"></a>1.6 目前主流的attention方法有哪些？</h3><p>讲自己熟悉的就可：</p>
<ul>
<li><strong>Scaled Dot-Product Attention</strong>: 这是Transformer模型中最常用的Attention机制，用于计算查询向量（Q）与键向量（K）之间的相似度得分，然后使用注意力权重对值向量（V）进行加权求和。</li>
<li><strong>Multi-Head Attention</strong>: 这是Transformer中的一个改进，通过同时使用多组独立的注意力头（多个QKV三元组），并在输出时将它们拼接在一起。这样的做法<strong>允许模型在不同的表示空间上学习不同类型的注意力模式</strong>。</li>
<li><strong>Relative Positional Encoding</strong>: 传统的Self-Attention机制在处理序列时并未直接考虑位置信息，而相对位置编码引入了位置信息，使得模型能够更好地处理序列中不同位置之间的关系。</li>
<li><strong>Transformer-XL</strong>: 一种改进的Transformer模型，通过使用循环机制来扩展Self-Attention的上下文窗口，从而处理更长的序列依赖性。</li>
</ul>
<h3 id="1-7-self-attention-在计算的过程中，如何对padding位做mask？"><a href="#1-7-self-attention-在计算的过程中，如何对padding位做mask？" class="headerlink" title="1.7 self-attention 在计算的过程中，如何对padding位做mask？"></a>1.7 self-attention 在计算的过程中，如何对padding位做mask？</h3><p>在 Attention 机制中，同样需要忽略 padding 部分的影响。</p>
<p>这里以transformer encoder中的self-attention为例：self-attention中，Q和K在点积之后，需要先经过mask再进行softmax，因此，<strong>对于要屏蔽的部分，mask之后的输出需要为负无穷</strong>，这样softmax之后输出才为0。</p>
<h3 id="1-8-深度学习中attention与全连接层的区别何在？"><a href="#1-8-深度学习中attention与全连接层的区别何在？" class="headerlink" title="1.8 深度学习中attention与全连接层的区别何在？"></a>1.8 深度学习中attention与全连接层的区别何在？</h3><p>这是个非常有意思的问题，要回答这个问题，我们必须重新定义一下Attention。</p>
<p>Transformer Paper里重新用QKV定义了Attention。所谓的QKV就是Query，Key，Value。如果我们用这个机制来研究传统的RNN attention，就会发现这个过程其实是这样的：RNN最后一步的output是Q，这个Q query了每一个中间步骤的K。Q和K共同产生了Attention Score，最后Attention Score乘以V加权求和得到context。那如果我们不用Attention，单纯用全连接层呢？很简单，全连接层可没有什么Query和Key的概念，只有一个Value，也就是说给每个V加一个权重再加到一起（如果是Self Attention，加权这个过程都免了，因为V就直接是从raw input加权得到的。）</p>
<p><strong>可见Attention和全连接最大的区别就是Query和Key</strong>，而这两者也恰好产生了Attention Score这个Attention中最核心的机制。<strong>而在Query和Key中，我认为Query又相对更重要，因为Query是一个锚点，Attention Score便是从过计算与这个锚点的距离算出来的</strong>。任何Attention based algorithm里都会有Query这个概念，但全连接显然没有。</p>
<p>最后来一个比较形象的比喻吧。如果一个神经网络的任务是从一堆白色小球中找到一个略微发灰的，那么全连接就是在里面随便乱抓然后凭记忆和感觉找，而attention则是左手拿一个白色小球，右手从袋子里一个一个抓出来，两两对比颜色，你左手抓的那个白色小球就是Query。</p>
<h2 id="2-Transformer"><a href="#2-Transformer" class="headerlink" title="2.Transformer"></a>2.Transformer</h2><h3 id="2-1-transformer中multi-head-attention中每个head为什么要进行降维？"><a href="#2-1-transformer中multi-head-attention中每个head为什么要进行降维？" class="headerlink" title="2.1 transformer中multi-head attention中每个head为什么要进行降维？"></a>2.1 transformer中multi-head attention中每个head为什么要进行降维？</h3><p>在Transformer的Multi-Head Attention中，对每个head进行降维是<strong>为了增加模型的表达能力和效率。</strong></p>
<p>每个head是独立的注意力机制，它们可以学习不同类型的特征和关系。通过使用多个注意力头，Transformer可以<strong>并行地学习多种不同的特征表示</strong>，从而增强了模型的表示能力。</p>
<p>然而，在使用多个注意力头的同时，注意力机制的计算复杂度也会增加。原始的Scaled Dot-Product Attention的计算复杂度为$O(d^2)$，其中d是输入向量的维度。如果使用h个注意力头，计算复杂度将增加到 $O(hd^2)$。这可能会导致Transformer在处理大规模输入时变得非常耗时。</p>
<p>为了缓解计算复杂度的问题，Transformer中在每个head上进行降维。在每个注意力头中，输入向量通过线性变换被映射到一个较低维度的空间。这个降维过程使用两个矩阵：一个是查询（Q）和键（K）的降维矩阵$W_q$和$W_k$​，另一个是值（V）的降维矩阵$W_v$​。</p>
<p>通过降低每个head的维度，Transformer可以在<strong>保持较高的表达能力的同时，大大减少计算复杂度</strong>。降维后的计算复杂度为$O(h\hat d ^ 2)$，其中$\hat d$是降维后的维度。通常情况下，$\hat d$会远小于原始维度d，这样就可以显著提高模型的计算效率。</p>
<h3 id="2-2-transformer在哪里做了权重共享，为什么可以做权重共享？"><a href="#2-2-transformer在哪里做了权重共享，为什么可以做权重共享？" class="headerlink" title="2.2 transformer在哪里做了权重共享，为什么可以做权重共享？"></a>2.2 transformer在哪里做了权重共享，为什么可以做权重共享？</h3><p>Transformer在Encoder和Decoder中都进行了权重共享。</p>
<p>在Transformer中，Encoder和Decoder是由多层的Self-Attention Layer和前馈神经网络层交叉堆叠而成。<strong>权重共享是指在这些堆叠的层中，相同位置的层共用相同的参数</strong>。</p>
<p>在Encoder中，所有的自注意力层和前馈神经网络层都共享相同的参数。这意味着每一层的自注意力机制和前馈神经网络都使用相同的权重矩阵来进行计算。这种共享保证了每一层都执行相同的计算过程，使得模型能够更好地捕捉输入序列的不同位置之间的关联性。</p>
<p>在Decoder中，除了和Encoder相同的权重共享方式外，还存在另一种特殊的权重共享：<strong>Decoder的自注意力层和Encoder的自注意力层之间也进行了共享</strong>。这种共享方式被称为”masked self-attention”，因为在解码过程中，当前位置的注意力不能关注到未来的位置（后续位置），以避免信息泄漏。通过这种共享方式，Decoder可以利用Encoder的表示来理解输入序列并生成输出序列。</p>
<p>权重共享的好处是大大减少了模型的参数数量，使得Transformer可以更有效地训练，并且更容易进行推理。此外，共享参数还有助于加快训练速度和提高模型的泛化能力，因为模型可以在不同位置共享并学习通用的特征表示。</p>
<h3 id="2-3-transformer的点积模型做缩放的原因是什么？"><a href="#2-3-transformer的点积模型做缩放的原因是什么？" class="headerlink" title="2.3 transformer的点积模型做缩放的原因是什么？"></a>2.3 transformer的点积模型做缩放的原因是什么？</h3><p>使用缩放的原因是为了控制注意力权重的尺度，以避免在计算过程中出现梯度爆炸的问题。</p>
<p>Attention的计算是在内积之后进行softmax，主要涉及的运算是$e^{q \cdot k}$，可以大致认为内积之后、softmax之前的数值在$-3\sqrt{d}$到$3\sqrt{d}$这个范围内，由于d通常都至少是64，所以$e^{3\sqrt{d}}$比较大而 $e^{-3\sqrt{d}}$比较小，因此经过softmax之后，Attention的分布非常接近一个one hot分布了，这带来严重的梯度消失问题，导致训练效果差。（例如y&#x3D;softmax(x)在|x|较大时进入了饱和区，x继续变化y值也几乎不变，即饱和区梯度消失）</p>
<p>相应地，解决方法就有两个:</p>
<ol>
<li>像NTK参数化那样，在内积之后除以 $\sqrt{d}$，使$q\cdot k$的方差变为1，对应$e^3,e^{−3}$都不至于过大过小，这样softmax之后也不至于变成one hot而梯度消失了，这也是常规的Transformer如BERT里边的Self Attention的做法</li>
<li>另外就是不除以 $\sqrt{d}$，但是初始化q,k的全连接层的时候，其初始化方差要多除以一个d，这同样能使得使$q\cdot k$的初始方差变为1，T5采用了这样的做法。</li>
</ol>
<h3 id="2-4-transformer-为什么使用-layer-normalization？batch-normalization-为什么不能用？"><a href="#2-4-transformer-为什么使用-layer-normalization？batch-normalization-为什么不能用？" class="headerlink" title="2.4 transformer 为什么使用 layer normalization？batch normalization 为什么不能用？"></a>2.4 transformer 为什么使用 layer normalization？batch normalization 为什么不能用？</h3><p>首先通过一个例子来说明layer norm和batch norm（只考虑均值归一化）是什么：</p>
<table>
<thead>
<tr>
<th>样本index</th>
<th>x1</th>
<th>x2</th>
<th>标签</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0</td>
<td>10</td>
<td>True</td>
</tr>
<tr>
<td>2</td>
<td>2</td>
<td>0</td>
<td>False</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>样本index</th>
<th>x1 (BN)</th>
<th>x2 (BN)</th>
<th>标签</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0 - 1 &#x3D; -1</td>
<td>10 - 5 &#x3D; 5</td>
<td>True</td>
</tr>
<tr>
<td>2</td>
<td>2 - 1 &#x3D; 1</td>
<td>0 - 5 &#x3D; -5</td>
<td>False</td>
</tr>
</tbody></table>
<table>
<thead>
<tr>
<th>样本index</th>
<th>x1 (LN)</th>
<th>x2 (LN)</th>
<th>标签</th>
</tr>
</thead>
<tbody><tr>
<td>1</td>
<td>0 - 5 &#x3D; -5</td>
<td>10 - 5 &#x3D; 5</td>
<td>True</td>
</tr>
<tr>
<td>2</td>
<td>2 - 1 &#x3D; 1</td>
<td>0 - 1 &#x3D; -1</td>
<td>False</td>
</tr>
</tbody></table>
<p>不同正则化方法的区别只是操作的信息维度不同，即选择损失信息的维度不同。</p>
<p>在CV中常常使用BN，它是在N、HW维度进行了归一化，而Channel维度的信息原封不动，因为可以认为在CV应用场景中，数据在不同channel中的信息很重要，如果对其进行归一化将会损失不同channel的差异信息。</p>
<p>而NLP中<strong>不同batch样本的信息关联性不大</strong>，而且由于<strong>不同的句子长度不同</strong>，强行归一化会损失不同样本间的差异信息，所以就没在batch维度进行归一化，而是选择LN，只考虑的句子内部维度的归一化。 可以认为NLP应用场景中一个样本内部维度间是有关联的，所以在信息归一化时，对样本内部差异信息进行一些损失，反而能降低方差。</p>
<h2 id="4-MHA-MQA-MGA"><a href="#4-MHA-MQA-MGA" class="headerlink" title="4.MHA &amp; MQA &amp; MGA"></a>4.MHA &amp; MQA &amp; MGA</h2><h3 id="（1）MHA"><a href="#（1）MHA" class="headerlink" title="（1）MHA"></a>（1）MHA</h3><p>从多头注意力的结构图中，貌似这个所谓的<strong>多个头就是指多组线性变换层</strong>，其实并不是，只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，<strong>这些变换不会改变原有张量的尺寸</strong>，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量。这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</p>
<p>Multi-head attention允许模型<strong>共同关注来自不同位置的不同表示子空间的信息</strong>，如果只有一个attention head，它的平均值会削弱这个信息。</p>
<p>$MultiHead(Q,K,V)&#x3D;Concat(head_1,…,head_h)W^O \ where ~ head_i &#x3D; Attention(QW_i^Q, KW_i^K, VW_i^V) $</p>
<p>其中映射由权重矩阵完成：<br>$W^Q_i \in \mathbb{R}^{d_ \times d_k}$<br>$W^K_i \in \mathbb{R}^{d_{\text{model}} \times d_k}$<br>$W^V_i \in \mathbb{R}^{d_{\text{model}} \times d_v}$<br>​和$W^O_i \in \mathbb{R}^{hd_v \times d_{\text{model}} }$</p>
<p><img src="https://dongnian.icu/media/image.png" srcset="/img/loading.gif" lazyload></p>
<p><img src="https://dongnian.icu/media/image_1.png" srcset="/img/loading.gif" lazyload></p>
<p><strong>多头注意力作用</strong>？</p>
<p>这种结构设计能<strong>让每个注意力机制去优化每个词汇的不同特征部分</strong>，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</p>
<p><strong>为什么要做多头注意力机制呢</strong>？</p>
<ul>
<li>一个 dot product 的注意力里面，没有什么可以学的参数。具体函数就是内积，为了识别不一样的模式，希望有不一样的计算相似度的办法。加性 attention 有一个权重可学，也许能学到一些内容。</li>
<li>multi-head attention 给 h 次机会去学习 不一样的投影的方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把 h 个 heads 拼接起来，最后再做一次投影。</li>
<li>每一个头 hi 是把 Q,K,V 通过 可以学习的 Wq, Wk, Wv 投影到 dv 上，再通过注意力函数，得到 headi。</li>
</ul>
<h3 id="（2）MQA"><a href="#（2）MQA" class="headerlink" title="（2）MQA"></a>（2）MQA</h3><p>MQA（Multi Query Attention）最早是出现在2019年谷歌的一篇论文 《Fast Transformer Decoding: One Write-Head is All You Need》。</p>
<p>MQA的思想其实比较简单，MQA 与 MHA 不同的是，<strong>MQA 让所有的头之间共享同一份 Key 和 Value 矩阵，每个头正常的只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量</strong>。</p>
<blockquote>
<p>Multi-query attention is identical except that the different heads share a single set of keys and values.</p>
</blockquote>
<p><img src="https://dongnian.icu/media/image_2.png" srcset="/img/loading.gif" lazyload></p>
<p>在 Multi-Query Attention 方法中只会保留一个单独的key-value头，这样<strong>虽然可以提升推理的速度，但是会带来精度上的损失</strong>。《Multi-Head Attention:Collaborate Instead of Concatenate 》这篇论文的第一个思路是<strong>基于多个 MQA 的 checkpoint 进行 finetuning，来得到了一个质量更高的 MQA 模型</strong>。这个过程也被称为 Uptraining。</p>
<p>具体分为两步：</p>
<ol>
<li>对多个 MQA 的 checkpoint 文件进行融合，融合的方法是: 通过对 key 和 value 的 head 头进行 mean pooling 操作，如下图。</li>
<li>对融合后的模型使用少量数据进行 finetune 训练，重训后的模型大小跟之前一样，但是效果会更好</li>
</ol>
<p><img src="https://dongnian.icu/media/image_3.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="（3）GQA"><a href="#（3）GQA" class="headerlink" title="（3）GQA"></a>（3）GQA</h3><p>Google 在 2023 年发表的一篇 <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13245.pdf">《GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints》</a>的论文</p>
<p>如下图所示，</p>
<ul>
<li>在 <strong>MHA(Multi Head Attention)</strong> 中，每个头有自己单独的 key-value 对；</li>
<li>在 <strong>MQA(Multi Query Attention)</strong> 中只会有一组 key-value 对；</li>
<li>在 <strong>GQA(Grouped Query Attention)</strong> 中，会对 attention 进行分组操作，query 被分为 N 组，每个组共享一个 Key 和 Value 矩阵。</li>
</ul>
<p><img src="https://dongnian.icu/media/image_4.png" srcset="/img/loading.gif" lazyload></p>
<p>GQA-N 是指具有 N 组的 Grouped Query Attention。GQA-1具有单个组，因此具有单个Key 和 Value，等效于MQA。而GQA-H具有与头数相等的组，等效于MHA。</p>
<p>在基于 Multi-head 多头结构变为 Grouped-query 分组结构的时候，也是采用跟上图一样的方法，对每一组的 key-value 对进行 mean pool 的操作进行参数融合。<strong>融合后的模型能力更综合，精度比 Multi-query 好，同时速度比 Multi-head 快</strong>。</p>
<p><img src="https://dongnian.icu/media/image_5.png" srcset="/img/loading.gif" lazyload></p>
<h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><p>MHA（Multi-head Attention）是标准的多头注意力机制，h个Query、Key 和 Value 矩阵。</p>
<p>MQA（Multi-Query Attention）是多查询注意力的一种变体，也是用于自回归解码的一种注意力机制。与MHA不同的是，<strong>MQA 让所有的头之间共享同一份 Key 和 Value 矩阵，每个头只单独保留了一份 Query 参数，从而大大减少 Key 和 Value 矩阵的参数量</strong>。</p>
<p>GQA（Grouped-Query Attention）是分组查询注意力，<strong>GQA将查询头分成G组，每个组共享一个Key 和 Value 矩阵</strong>。GQA-G是指具有G组的grouped-query attention。GQA-1具有单个组，因此具有单个Key 和 Value，等效于MQA。而GQA-H具有与头数相等的组，等效于MHA。</p>
<p>GQA介于MHA和MQA之间。GQA 综合 MHA 和 MQA ，既不损失太多性能，又能利用 MQA 的推理加速。不是所有 Q 头共享一组 KV，而是分组一定头数 Q 共享一组 KV，比如上图中就是两组 Q 共享一组 KV。</p>
<p><img src="https://dongnian.icu/media/image_6.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="5-Flash-Attention"><a href="#5-Flash-Attention" class="headerlink" title="5.Flash Attention"></a>5.Flash Attention</h2><p>论文名称：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></p>
<p>Flash Attention的主要目的是加速和节省内存，主要贡献包括：</p>
<ul>
<li>计算softmax时候不需要全量input数据，可以分段计算；</li>
<li>反向传播的时候，不存储attention matrix (N^2的矩阵)，而是只存储softmax归一化的系数。</li>
</ul>
<h3 id="5-1-动机"><a href="#5-1-动机" class="headerlink" title="5.1 动机"></a>5.1 动机</h3><p>不同硬件模块之间的带宽和存储空间有明显差异，例如下图中左边的三角图，最顶端的是GPU种的<code>SRAM</code>，它的容量非常小但是带宽非常大，以A100 GPU为例，它有108个流式多核处理器，每个处理器上的片上SRAM大小只有192KB，因此A100总共的SRAM大小是192KB×108&#x3D;20MB192KB\times 108 &#x3D; 20MB192KB×108&#x3D;20MB，但是其吞吐量能高达19TB&#x2F;s。而A100 GPU <code>HBM</code>（High Bandwidth Memory也就是我们常说的GPU显存大小）大小在40GB~80GB左右，但是带宽只与1.5TB&#x2F;s。</p>
<p><img src="https://dongnian.icu/media/image_7.png" srcset="/img/loading.gif" lazyload></p>
<p>下图给出了标准的注意力机制的实现流程，可以看到因为<code>HBM</code>的大小更大，<strong>我们平时写pytorch代码的时候最常用到的就是HBM，所以对于HBM的读写操作非常频繁，而SRAM利用率反而不高</strong>。</p>
<p><img src="https://dongnian.icu/media/image_8.png" srcset="/img/loading.gif" lazyload></p>
<p>FlashAttention的主要动机就是<strong>希望把SRAM利用起来</strong>，但是难点就在于SRAM太小了，一个普通的矩阵乘法都放不下去。FlashAttention的解决思路就是将计算模块进行分解，拆成一个个小的计算任务。</p>
<h3 id="5-2-Softmax-Tiling"><a href="#5-2-Softmax-Tiling" class="headerlink" title="5.2 Softmax Tiling"></a>5.2 Softmax Tiling</h3><p>在介绍具体的计算算法前，我们首先需要了解一下Softmax Tiling。</p>
<p>（1）数值稳定</p>
<p>Softmax包含指数函数，所以为了避免数值溢出问题，可以将每个元素都减去最大值，如下图示，最后计算结果和原来的Softmax是一致的。</p>
<p>$m(x):&#x3D;\max <em>{i} ~ x</em>{i} \ f(x):\left[\begin{array}{llll}e^{x_{1}-m(x)} &amp; \ldots &amp; e^{x_{B}-m(x)}\end{array}\right] \ \ell(x):&#x3D;\sum <em>{i} f(x)</em>{i} \ \operatorname{softmax}(x):&#x3D;\frac{f(x)}{\ell(x)}$</p>
<p>（2）分块计算softmax</p>
<p>因为Softmax都是按行计算的，所以我们考虑一行切分成两部分的情况，即原本的一行数据$x \in \mathbb{R}^{2 B}&#x3D;\left[x^{(1)}, x^{(2)}\right]$</p>
<p><img src="https://dongnian.icu/media/image_9.png" srcset="/img/loading.gif" lazyload></p>
<p>可以看到计算不同块的f(x)值时，乘上的系数是不同的，但是最后化简后的结果都是指数函数减去了整行的最大值。以$x^{(1)}$ 为例，</p>
<p>$<br>\begin{aligned} m^{m\left(x^{(1)}\right)-m(x)} f\left(x^{(1)}\right) &amp; &#x3D;e^{m\left(x^{(1)}\right)-m(x)}\left[e^{x_{1}^{(1)}-m\left(x^{(1)}\right)}, \ldots, e^{x_{B}^{(1)}-m\left(x^{(1)}\right)}\right] \ &amp; &#x3D;\left[e^{x_{1}^{(1)}-m(x)}, \ldots, e^{x_{B}^{(1)}-m(x)}\right]\end{aligned}​​​​​<br>​$</p>
<h3 id="5-3-算法流程"><a href="#5-3-算法流程" class="headerlink" title="5.3 算法流程"></a>5.3 算法流程</h3><p>FlashAttention旨在避免从 HBM（High Bandwidth Memory）中读取和写入注意力矩阵，这需要做到：</p>
<ol>
<li>目标一：在不访问整个输入的情况下计算softmax函数的缩减；<strong>将输入分割成块，并在输入块上进行多次传递，从而以增量方式执行softmax缩减</strong>。</li>
<li>目标二：在后向传播中不能存储中间注意力矩阵。标准Attention算法的实现需要将计算过程中的S、P写入到HBM中，而这些中间矩阵的大小与输入的序列长度有关且为二次型，因此<strong>Flash Attention就提出了不使用中间注意力矩阵，通过存储归一化因子来减少HBM内存的消耗。</strong></li>
</ol>
<p>FlashAttention算法流程如下图所示：</p>
<p><img src="https://dongnian.icu/media/image_10.png" srcset="/img/loading.gif" lazyload></p>
<p>为方便理解，下图将FlashAttention的计算流程可视化出来了，简单理解就是每一次只计算一个block的值，通过多轮的双for循环完成整个注意力的计算。</p>
<p><img src="https://dongnian.icu/media/image_11.png" srcset="/img/loading.gif" lazyload></p>
<h2 id="6-Transformer常见问题"><a href="#6-Transformer常见问题" class="headerlink" title="6.Transformer常见问题"></a>6.Transformer常见问题</h2><h3 id="6-1-Transformer和RNN"><a href="#6-1-Transformer和RNN" class="headerlink" title="6.1 Transformer和RNN"></a>6.1 Transformer和RNN</h3><p>最简单情况：没有残差连接、没有 layernorm、 attention 单头、没有投影。看和 RNN 区别</p>
<ul>
<li>attention 对输入做一个加权和，加权和 进入 point-wise MLP。（画了多个红色方块 MLP， 是一个权重相同的 MLP）</li>
<li>point-wise MLP 对 每个输入的点 做计算，得到输出。</li>
<li>attention 作用：把整个序列里面的信息抓取出来，做一次汇聚 aggregation</li>
</ul>
<p><img src="https://dongnian.icu/media/image_12.png" srcset="/img/loading.gif" lazyload></p>
<p>RNN 跟 transformer <strong>异：如何传递序列的信</strong>息</p>
<p>RNN 是把上一个时刻的信息输出传入下一个时候做输入。Transformer 通过一个 attention 层，去全局的拿到整个序列里面信息，再用 MLP 做语义的转换。</p>
<p>RNN 跟 transformer <strong>同：语义空间的转换 + 关注点</strong></p>
<p>用一个线性层 or 一个 MLP 来做语义空间的转换。</p>
<p><strong>关注点</strong>：怎么有效的去使用序列的信息。</p>
<h3 id="6-2-一些细节"><a href="#6-2-一些细节" class="headerlink" title="6.2 一些细节"></a>6.2 一些细节</h3><p><strong>Transformer为何使用多头注意力机制？</strong>（为什么不使用一个头）</p>
<ul>
<li>多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。可以类比CNN中同时使用<strong>多个滤波器</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征&#x2F;信息。</strong></li>
</ul>
<p><strong>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</strong> （注意和第一个问题的区别）</p>
<ul>
<li>使用Q&#x2F;K&#x2F;V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</li>
<li>同时，由softmax函数的性质决定，实质做的是一个soft版本的arg max操作，得到的向量接近一个one-hot向量（接近程度根据这组数的数量级有所不同）。如果令Q&#x3D;K，那么得到的模型大概率会得到一个类似单位矩阵的attention矩阵，<strong>这样self-attention就退化成一个point-wise线性映射</strong>。这样至少是违反了设计的初衷。</li>
</ul>
<p><strong>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</strong></p>
<ul>
<li>K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的$W_k, W_Q$来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。</li>
<li>为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。</li>
</ul>
<p><strong>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）</strong>，并使用公式推导进行讲解</p>
<ul>
<li>这取决于softmax函数的特性，如果softmax内计算的数数量级太大，会输出近似one-hot编码的形式，导致梯度消失的问题，所以需要scale</li>
<li>那么至于为什么需要用维度开根号，假设向量q，k满足各分量独立同分布，均值为0，方差为1，那么qk点积均值为0，方差为dk，从统计学计算，若果让qk点积的方差控制在1，需要将其除以dk的平方根，是的softmax更加平滑</li>
</ul>
<p><strong>在计算attention score的时候如何对padding做mask操作？</strong></p>
<ul>
<li>padding位置置为负无穷(一般来说-1000就可以)，再对attention score进行相加。对于这一点，涉及到batch_size之类的，具体的大家可以看一下实现的源代码，位置在这里：<a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720">https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720</a></li>
<li>padding位置置为负无穷而不是0，是因为后续在softmax时，$e^0&#x3D;1$，不是0，计算会出现错误；而$e^{-\infty} &#x3D; 0$，所以取负无穷</li>
</ul>
<p><strong>为什么在进行多头注意力的时候需要对每个head进行降维？</strong>（可以参考上面一个问题）</p>
<ul>
<li>将原有的<strong>高维空间转化为多个低维空间</strong>并再最后进行拼接，形成同样维度的输出，借此丰富特性信息<ul>
<li>基本结构：Embedding + Position Embedding，Self-Attention，Add + LN，FN，Add + LN</li>
</ul>
</li>
</ul>
<p><strong>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？</strong></p>
<ul>
<li>embedding matrix的初始化方式是xavier init，这种方式的方差是1&#x2F;embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</li>
</ul>
<p><strong>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</strong></p>
<ul>
<li>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</li>
</ul>
<p><strong>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong>（参考上一题）</p>
<ul>
<li>相对位置编码（RPE）1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</li>
</ul>
<p><strong>简单讲一下Transformer中的残差结构以及意义。</strong></p>
<ul>
<li>就是ResNet的优点，解决梯度消失</li>
</ul>
<p><strong>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</strong></p>
<ul>
<li>LN：针对每个样本序列进行Norm，没有样本间的依赖。对一个序列的不同特征维度进行Norm</li>
<li>CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为<strong>句子长度不一致，并且各个batch的信息没什么关系</strong>，因此只考虑句子内信息的归一化，也就是LN。</li>
</ul>
<p><strong>简答讲一下BatchNorm技术，以及它的优缺点。</strong></p>
<ul>
<li>优点：<ul>
<li>第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使<strong>损失平面更加的平滑</strong>，从而加快的收敛速度。<ul>
<li>第二个优点就是缓解了<strong>梯度饱和问题</strong>（如果使用sigmoid激活函数的话），加快收敛。</li>
</ul>
</li>
</ul>
</li>
<li>缺点：<ul>
<li>第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用 整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size 较小的时候，效果肯定不好。</li>
<li>第二个缺点就是 BN 在RNN中效果比较差。</li>
</ul>
</li>
</ul>
<p><strong>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</strong></p>
<ul>
<li>ReLU</li>
</ul>
<p>$FFN(x)&#x3D;max(0,~ xW_1+b_1)W_2+b_2$</p>
<p><strong>Encoder端和Decoder端是如何进行交互的？</strong>（在这里可以问一下关于seq2seq的attention知识）</p>
<ul>
<li>Cross Self-Attention，Decoder提供Q，Encoder提供K，V</li>
</ul>
<p><strong>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</strong>（为什么需要decoder自注意力需要进行 sequence mask）</p>
<ul>
<li>让输入序列只看到过去的信息，不能让他看到未来的信息</li>
</ul>
<p><strong>Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</strong></p>
<ul>
<li>Encoder侧：模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。</li>
<li>Decode引入sequence mask就是为了并行化训练，Decoder推理过程没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。</li>
</ul>
<p><strong>简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</strong></p>
<ul>
<li>传统词表示方法无法很好的处理未知或罕见的词汇（OOV问题），传统词tokenization方法不利于模型学习词缀之间的关系”</li>
<li>BPE（字节对编码）或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。后期使用时需要一个替换表来重建原始数据。</li>
<li>优点：可以有效地平衡词汇表大小和步数（编码句子所需的token次数）。</li>
<li>缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</li>
</ul>
<p><strong>Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</strong></p>
<ul>
<li>Dropout测试的时候记得对输入整体呈上dropout的比率</li>
</ul>
<p><strong>引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</strong></p>
<ul>
<li>BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</li>
</ul>
<p>本文转自 <a target="_blank" rel="noopener" href="https://dongnian.icu/note/llm/llm_concept/02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80/1.attention/1.attention.html">https://dongnian.icu/note/llm/llm_concept&#x2F;02.%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80&#x2F;1.attention&#x2F;1.attention.html</a>，如有侵权，请联系删除。</p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E7%AC%94%E8%AE%B0/" class="category-chain-item">笔记</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Cheatsheet/" class="print-no-link">#Cheatsheet</a>
      
        <a href="/tags/Transformer/" class="print-no-link">#Transformer</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>笔记 - Transformer面试题目总结</div>
      <div>https://lcj2021.github.io/2024/04/16/笔记 - Transformer面试题目总结/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Channing Lau</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年4月16日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/04/16/%E7%AC%94%E8%AE%B0%20-%20Transformer%E5%9B%BE%E8%A7%A3/" title="笔记 - Transformer图解">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">笔记 - Transformer图解</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/04/14/%E7%AC%94%E8%AE%B0%20-%20%E9%80%9A%E7%94%A8%E9%9D%A2%E8%AF%95%E6%89%8B%E7%A8%BF/" title="笔记 - 通用面试手稿">
                        <span class="hidden-mobile">笔记 - 通用面试手稿</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
