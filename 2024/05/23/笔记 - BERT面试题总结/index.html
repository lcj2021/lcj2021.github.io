

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=dark>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Channing Lau">
  <meta name="keywords" content="">
  
    <meta name="description" content="3.1 BERT用字粒度、词粒度和子词粒度的优缺点有哪些？BERT可以使用字粒度（character-level）和词粒度（word-level）两种方式来进行文本表示，它们各自有优缺点： 字粒度（Character-level）：  优点：处理未登录词（Out-of-Vocabulary，OOV）：字粒度可以处理任意字符串，包括未登录词，不需要像词粒度那样遇到未登录词就忽略或使用特殊标记。对于少">
<meta property="og:type" content="article">
<meta property="og:title" content="笔记 - BERT面试题目总结">
<meta property="og:url" content="https://lcj2021.github.io/2024/05/23/%E7%AC%94%E8%AE%B0%20-%20BERT%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/index.html">
<meta property="og:site_name" content="ChanningLau&#39;s harbour">
<meta property="og:description" content="3.1 BERT用字粒度、词粒度和子词粒度的优缺点有哪些？BERT可以使用字粒度（character-level）和词粒度（word-level）两种方式来进行文本表示，它们各自有优缺点： 字粒度（Character-level）：  优点：处理未登录词（Out-of-Vocabulary，OOV）：字粒度可以处理任意字符串，包括未登录词，不需要像词粒度那样遇到未登录词就忽略或使用特殊标记。对于少">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-05-22T16:00:00.000Z">
<meta property="article:modified_time" content="2024-07-06T11:45:10.854Z">
<meta property="article:author" content="Channing Lau">
<meta property="article:tag" content="Cheatsheet">
<meta property="article:tag" content="BERT">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>笔记 - BERT面试题目总结 - ChanningLau&#39;s harbour</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"lcj2021.github.io","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.1.1"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ChanningLau&#39;s harbour</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="笔记 - BERT面试题目总结"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-05-23 00:00" pubdate>
          2024年5月23日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          3.4k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          29 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">笔记 - BERT面试题目总结</h1>
            
            
              <div class="markdown-body">
                
                <h2 id="3-1-BERT用字粒度、词粒度和子词粒度的优缺点有哪些？"><a href="#3-1-BERT用字粒度、词粒度和子词粒度的优缺点有哪些？" class="headerlink" title="3.1 BERT用字粒度、词粒度和子词粒度的优缺点有哪些？"></a>3.1 BERT用字粒度、词粒度和子词粒度的优缺点有哪些？</h2><p>BERT可以使用字粒度（character-level）和词粒度（word-level）两种方式来进行文本表示，它们各自有优缺点：</p>
<p>字粒度（Character-level）：</p>
<ul>
<li><strong>优点</strong>：处理未登录词（Out-of-Vocabulary，OOV）：字粒度可以处理任意字符串，包括未登录词，不需要像词粒度那样遇到未登录词就忽略或使用特殊标记。对于少见词和低频词，字粒度可以学习更丰富的字符级别表示，使得模型能够更好地捕捉词汇的细粒度信息。词表会大大减小，26个字母基本就能够覆盖所有词，5000多个中文基本也能组合覆盖的词汇。</li>
<li><strong>缺点</strong>：计算复杂度高：使用字粒度会导致输入序列的长度大大增加，进而增加模型的计算复杂度和内存消耗。需要更多的训练数据：字粒度模型对于少见词和低频词需要更多的训练数据来学习有效的字符级别表示，否则可能会导致过拟合。</li>
</ul>
<p>词粒度（Word-level）：</p>
<ul>
<li><strong>优点</strong>：计算效率高：使用词粒度可以大大减少输入序列的长度，从而降低模型的计算复杂度和内存消耗。学习到更加稳定的词级别表示：词粒度模型可以学习到更加稳定的词级别表示，特别是对于高频词和常见词，有更好的表示能力。</li>
<li><strong>缺点</strong>：处理未登录词（OOV）：词粒度模型无法处理未登录词，遇到未登录词时需要采用特殊处理（如使用未登录词的特殊标记或直接忽略）；对于多音字等形态复杂的词汇，可能无法准确捕捉其细粒度的信息；词表中的低频词&#x2F;稀疏词在模型训练过程中无法得到充分训练，进而模型不能充分理解这些词的语义</li>
</ul>
<p>子词粒度（Subword-level）：</p>
<p>对于未见单词，既不把它存为单独词表，也不按字母分词，而是把它拆分成有意义的子词单元<br>例如：unfriendly -&gt; un &#x2F; friend &#x2F; ly</p>
<ul>
<li><strong>优点</strong>：融合了字粒度和词粒度的优点。处理未登录词（OOV）：子词粒度模型可以处理未登录词，因为可以将其拆分为多个子词，从而学习到更加准确的子词级别表示。对于多音字等形态复杂的词汇，可以更好地捕捉其细粒度信息。</li>
<li><strong>缺点</strong>：只适用于英文等拉丁语系的语言，对于中文来讲，由于不能将某一个字拆分为偏旁部首和字根，没有意义。</li>
</ul>
<h2 id="3-BERT的三个embedding为什么可以直接相加？"><a href="#3-BERT的三个embedding为什么可以直接相加？" class="headerlink" title="3. BERT的三个embedding为什么可以直接相加？"></a>3. BERT的三个embedding为什么可以直接相加？</h2><p>三个embedding分别是：</p>
<ol>
<li>词嵌入（Word Embedding）：表示输入文本中每个词语的嵌入向量。这些嵌入向量可以通过预训练的词向量模型（如Word2Vec或GloVe）得到，也可以在BERT的训练过程中进行学习。</li>
<li>位置嵌入（Positional Embedding）：用于表示输入序列中每个词语的位置信息。由于BERT是基于Transformer的模型，并没有显式地编码位置信息，因此需要通过位置嵌入来提供序列中每个位置的特定表示。</li>
<li>分段嵌入（Segment Embedding）：用于区分不同句子或文本段落之间的嵌入。在处理包含多个句子的输入时，为了区分它们，BERT引入了分段嵌入。</li>
</ol>
<p>解释1：embedding的本质是one-hot向量经过全连接网络得到的向量。<br>三个 embedding 相加等价于三个原始 one-hot 的拼接再经过一个全连接网络。和拼接相比，相加可以节约模型参数。<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/374835153/answer/2108264167">从网络角度理解</a></p>
<p>解释2：在实际场景中，叠加是一个更为常态的操作。比如声音、图像等信号。一个时序的波可以用多个不司频率的正弦波善加来表示。只要着加的波的频率不同，我们就可以通过傅里叶变换进行逆向转换。<br>一串文本也可以看作是一些时序信号，也可以有很多信号进行叠加，只要频率不同都可以在后面的复杂神经网络中得到解耦(但也不一定真的要得到解耦)。在BERT这个设定中，符号，片段，位置明显可以对应三种非常不同的频率。<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/374835153/answer/1080315948">从信号角度理解</a></p>
<h2 id="3-2-BERT的Encoder与Decoder掩码有什么区别？"><a href="#3-2-BERT的Encoder与Decoder掩码有什么区别？" class="headerlink" title="3.2 BERT的Encoder与Decoder掩码有什么区别？"></a>3.2 BERT的Encoder与Decoder掩码有什么区别？</h2><p>Encoder主要使用自注意力掩码和填充掩码，而Decoder除了自注意力掩码外，还需要使用编码器-解码器注意力掩码来避免未来位置信息的泄露。这些掩码操作保证了Transformer在处理自然语言序列时能够准确、有效地进行计算，从而获得更好的表现。</p>
<h2 id="3-3-BERT用的是transformer里面的encoder还是decoder？"><a href="#3-3-BERT用的是transformer里面的encoder还是decoder？" class="headerlink" title="3.3 BERT用的是transformer里面的encoder还是decoder？"></a>3.3 BERT用的是transformer里面的encoder还是decoder？</h2><p>BERT使用的是Transformer中的<strong>Encoder部分</strong>，而不是Decoder部分。</p>
<p>Transformer模型由Encoder和Decoder两个部分组成。Encoder用于将输入序列编码为一系列高级表示，而Decoder用于基于这些表示生成输出序列。</p>
<p>在BERT模型中，只使用了Transformer的Encoder部分，并且对其进行了一些修改和自定义的预训练任务，而没有使用Transformer的Decoder部分。</p>
<h2 id="3-4-为什么BERT选择mask掉15-这个比例的词，可以是其他的比例吗？"><a href="#3-4-为什么BERT选择mask掉15-这个比例的词，可以是其他的比例吗？" class="headerlink" title="3.4 为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？"></a>3.4 为什么BERT选择mask掉15%这个比例的词，可以是其他的比例吗？</h2><p>BERT采用的Masked LM，会选取语料中所有词的15%进行随机mask，论文中表示是受到完形填空任务的启发，但其实与CBOW也有异曲同工之妙。</p>
<p>从CBOW的角度，这里 有一个比较好的解释是：在一个大小为 数学公式: $1 &#x2F; p&#x3D;100 &#x2F; 15 \approx 7$ 的窗口中随机选一个词，类似CBOW中滑动窗口的中心词，区别是这里的滑动窗口是非重叠的。</p>
<p>那从CBOW的滑动窗口角度，10%~20%都是还ok的比例。</p>
<p>15%是一种经验性的选择，是原论文中的一种选择，并没有一个固定的理论依据，实际中当然可以尝试不同的比例，15%的比例是由BERT的作者在原始论文中提出，并在实验中发现对于BERT的训练效果是有效的。</p>
<h2 id="3-在BERT中，token分3种情况做mask，分别是什么？作用是什么？"><a href="#3-在BERT中，token分3种情况做mask，分别是什么？作用是什么？" class="headerlink" title="3. 在BERT中，token分3种情况做mask，分别是什么？作用是什么？"></a>3. 在BERT中，token分3种情况做mask，分别是什么？作用是什么？</h2><p>15%token做mask；其中80%用[MASK]替换，10%用random token替换，10%不变。</p>
<ul>
<li>80%的概率替换成[MASK]，比如my dog is hairy → my dog is [MASK]</li>
<li>10%的概率替换成随机的一个词，比如my dog is hairy → my dog is apple</li>
<li>10%的概率替换成它本身，比如my dog is hairy → my dog is hairy</li>
</ul>
<p>其实这个就是典型的Denosing Autoencoder的思路，那些被Mask掉的单词就是在输入侧加入的所谓噪音。主要原因是：</p>
<p>(1) 在后续finetune任务中语句中并不会出现 [MASK] 标记，“不总是用实际的[MASK]标记替换被掩码的词”能减少预训练和微调之间的不匹配，使模型更好地适应微调任务的特定上下文；<br>(2) 预测一个词汇时，模型并不知道输入对应位置的词汇是否为正确的词汇（ 10% 概率），这就迫使模型更多地依赖于上下文信息去预测词汇，并且赋予了模型一定的纠错能力。</p>
<h2 id="3-5-为什么BERT在第一句前会加一个-CLS-标志"><a href="#3-5-为什么BERT在第一句前会加一个-CLS-标志" class="headerlink" title="3.5 为什么BERT在第一句前会加一个[CLS] 标志?"></a>3.5 为什么BERT在第一句前会加一个[CLS] 标志?</h2><p>BERT在第一句前会加一个 [CLS] 标志，<strong>最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等</strong>。为什么选它？因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。</p>
<p>具体来说，self-attention是用文本中的其它词来增强目标词的语义表示，但是目标词本身的语义还是会占主要部分的，因此，经过BERT的12层，每次词的embedding融合了所有词的信息，可以去更好的表示自己的语义。而 [CLS] 位<strong>本身没有语义，经过12层，得到的是attention后所有词的加权平均，相比其他正常词，可以更好的表征句子语义</strong>。</p>
<h2 id="3-6-BERT非线性的来源在哪里？"><a href="#3-6-BERT非线性的来源在哪里？" class="headerlink" title="3.6 BERT非线性的来源在哪里？"></a>3.6 BERT非线性的来源在哪里？</h2><p>主要来自两个地方：<strong>前馈层的gelu激活函数</strong>和<strong>self-attention</strong>。</p>
<p><strong>前馈神经网络层</strong>：在BERT的Encoder中，每个自注意力层之后都跟着一个前馈神经网络层。前馈神经网络层是全连接的神经网络，通常包括一个线性变换和一个非线性的激活函数，如gelu。这样的非线性激活函数引入了非线性变换，使得模型能够学习更加复杂的特征表示。</p>
<p><strong>self-attention layer</strong>：在自注意力层中，查询（Query）、键（Key）、值（Value）之间的点积得分会经过softmax操作，形成注意力权重，然后将这些权重与值向量相乘得到每个位置的自注意输出。这个过程中涉及了softmax操作，使得模型的计算是非线性的。</p>
<h2 id="3-7-BERT训练时使用的学习率warm-up和decay-策略是怎样的？为什么要这么做？"><a href="#3-7-BERT训练时使用的学习率warm-up和decay-策略是怎样的？为什么要这么做？" class="headerlink" title="3.7 BERT训练时使用的学习率warm-up和decay 策略是怎样的？为什么要这么做？"></a>3.7 BERT训练时使用的学习率warm-up和decay 策略是怎样的？为什么要这么做？</h2><p>在BERT的训练中，使用了学习率warm-up策略，这是<strong>为了在训练的早期阶段增加学习率，以提高训练的稳定性和加快模型收敛</strong>。</p>
<p>学习率warm-up策略的具体做法是，在训练开始的若干个步骤（通常是一小部分训练数据的迭代次数）内，<strong>将学习率逐渐从一个较小的初始值增加到预定的最大学习率，最后再decay到比较小的值</strong>。在这个过程中，学习率的变化是线性的，即学习率在warm-up阶段的每个步骤按固定的步幅逐渐增加。学习率warm-up的目的是为了解决BERT在训练初期的两个问题：</p>
<ul>
<li><strong>不稳定性</strong>：在训练初期，由于模型参数的随机初始化以及模型的复杂性，模型可能处于一个较不稳定的状态。此时使用较大的学习率可能导致模型的参数变动太大，使得模型很难收敛，学习率warm-up可以在这个阶段将学习率保持较小，提高模型训练的稳定性。</li>
<li><strong>避免过拟合</strong>：BERT模型往往需要较长的训练时间来获得高质量的表示。如果在训练的早期阶段就使用较大的学习率，可能会导致模型在训练初期就过度拟合训练数据，降低模型的泛化能力。通过学习率warm-up，在训练初期使用较小的学习率，可以避免过度拟合，等模型逐渐稳定后再使用较大的学习率进行更快的收敛。</li>
<li><strong>后期decay</strong>：当模型训到一定阶段后（比如十个epoch），模型的分布就已经比较固定了，或者说能学到的新东西就比较少了。如果还沿用较大的学习率，就会破坏这种稳定性，用我们通常的话说，就是已经接近loss的local optimal了，为了靠近这个point，我们就要降低学习率让模型在训练的后期细致地调整参数。</li>
</ul>
<h2 id="3-8-在BERT应用中，如何解决长文本问题？"><a href="#3-8-在BERT应用中，如何解决长文本问题？" class="headerlink" title="3.8 在BERT应用中，如何解决长文本问题？"></a>3.8 在BERT应用中，如何解决长文本问题？</h2><p>在BERT应用中，处理长文本问题有以下几种常见的解决方案：</p>
<ul>
<li><strong>截断与填充</strong>：将长文本截断为固定长度或者进行填充。BERT模型的输入是一个固定长度的序列，因此当输入的文本长度超过模型的最大输入长度时，需要进行截断或者填充。通常，可以根据任务的要求，选择适当的最大长度，并对文本进行截断或者填充，使其满足模型输入的要求。</li>
<li><strong>Sliding Window</strong>：将长文本分成多个短文本，然后分别输入BERT模型。这种方法被称为Sliding Window技术。具体来说，将长文本按照固定的步长切分成多个片段，然后分别输入BERT模型进行处理。每个片段的输出可以进行进一步的汇总或者融合，得到最终的表示。</li>
<li><strong>Hierarchical Model</strong>：使用分层模型来处理长文本，其中底层模型用于处理短文本片段，然后将不同片段的表示进行汇总或者融合得到整个长文本的表示。这样的分层模型可以充分利用BERT模型的表示能力，同时处理长文本。</li>
<li><strong>Longformer、BigBird等模型</strong>：使用专门针对长文本的模型，如Longformer和BigBird。这些模型采用了不同的注意力机制，以处理超长序列，并且通常在处理长文本时具有更高的效率。</li>
<li><strong>Document-Level Model</strong>：将文本看作是一个整体，而不是将其拆分成句子或段落，然后输入BERT模型进行处理。这样的文档级模型可以更好地捕捉整个文档的上下文信息，但需要更多的计算资源。</li>
</ul>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E7%AC%94%E8%AE%B0/" class="category-chain-item">笔记</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/Cheatsheet/" class="print-no-link">#Cheatsheet</a>
      
        <a href="/tags/BERT/" class="print-no-link">#BERT</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>笔记 - BERT面试题目总结</div>
      <div>https://lcj2021.github.io/2024/05/23/笔记 - BERT面试题总结/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Channing Lau</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2024年5月23日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/06/30/%E5%91%A8%E8%AE%B0%20-%202024%2006.24-06.30%EF%BC%88Tiktok%E5%85%A5%E8%81%8C%E7%AC%AC%E4%B8%80%E5%91%A8%EF%BC%89/" title="周记 - 2024 06.24-06.30">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">周记 - 2024 06.24-06.30</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/04/22/%E5%91%A8%E8%AE%B0%20-%202024%2004.29-05.05/" title="周记 - 2024 04.29-05.05">
                        <span class="hidden-mobile">周记 - 2024 04.29-05.05</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
